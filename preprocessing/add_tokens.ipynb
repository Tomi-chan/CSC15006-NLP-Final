{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model v√† th√™m nh·ªØng tokens ch·ªØ N√¥m v√†o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (BertTokenizer, BertForMaskedLM)\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "ancient_chinese_tokenizer = BertTokenizer.from_pretrained('Jihuai/bert-ancient-chinese') # fast_tokenizer=False\n",
    "ancient_chinese_model = BertForMaskedLM.from_pretrained('Jihuai/bert-ancient-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(38208, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=38208, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancient_chinese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='Jihuai/bert-ancient-chinese', vocab_size=38208, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancient_chinese_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 100, 7887, 7888, 102]\n",
      "[CLS] [UNK] È∏¶ È∏® [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Tr∆∞·ªõc khi th√™m tokens m·ªõi th√¨ m·ªôt s·ªë k√Ω t·ª± ch·ªØ N√¥m s·∫Ω ko encode ƒë∆∞·ª£c\n",
    "test_text = \"Û±™∫®∞àÛ±Æ∑Û∞îÉÛ∞≠ãÈ∏¶È∏®\"\n",
    "\"\"\" encoded = ancient_chinese_tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True\n",
    ") \"\"\"\n",
    "encoding = ancient_chinese_tokenizer.encode(test_text, add_special_tokens=True)  \n",
    "print(encoding)\n",
    "decode = ancient_chinese_tokenizer.decode(encoding)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T·∫°o t·∫≠p vocab_nom bao g·ªìm c√°c ch·ªØ trong vocab ban ƒë·∫ßu v√† th√™m c·∫£ c√°c ch·ªØ N√¥m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_nom = []\n",
    "with open(\"vocab_Han_Nom.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read().splitlines()\n",
    "    for i in data:\n",
    "        vocab_nom.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50424\n"
     ]
    }
   ],
   "source": [
    "vocab_model = ancient_chinese_tokenizer.get_vocab().keys()\n",
    "vocab_nom = list(vocab_model)+list(set(vocab_nom) - set(vocab_model))\n",
    "print(len(vocab_nom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Th√™m tokens m·ªõi v√†o tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens added: 12212\n"
     ]
    }
   ],
   "source": [
    "# Th√™m token m·ªõi\n",
    "num_added_tokens = ancient_chinese_tokenizer.add_tokens(vocab_nom)\n",
    "# In s·ªë token ƒë√£ th√™m th√†nh c√¥ng\n",
    "print(f\"Number of tokens added: {num_added_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [101, 5454, 3176, 22735, 850, 29685, 49808, 5765, 102]\n",
      "Decoded Text: [CLS] ËÄ® Êñº Âªõ ‰ºΩ Óáï ®¥¶ Ëåπ [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra encode v√† decode\n",
    "#test_text = \"Û±™∫®∞àÛ±Æ∑Û∞îÉÛ∞≠ãÈ∏¶È∏®\"\n",
    "test_text = \"ËÄ® Êñº Âªõ ‰ºΩ Óáï ®¥¶ Ëåπ\"\n",
    "#encoded = ancient_chinese_tokenizer.encode(test_text, add_special_tokens=True)\n",
    "#decoded = ancient_chinese_tokenizer.decode(encoded, add_special_tokens=True)\n",
    "\n",
    "encoded = ancient_chinese_tokenizer.encode(test_text, add_special_tokens=True)\n",
    "decoded = ancient_chinese_tokenizer.decode(encoded, add_special_tokens=True)\n",
    "print(\"Encoded IDs:\", encoded)\n",
    "print(\"Decoded Text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4185, 29685, 4187, 4188, 4189]\n"
     ]
    }
   ],
   "source": [
    "print(ancient_chinese_tokenizer.convert_tokens_to_ids([\"ÁÑï\", \"Óáï\", \"ÁÑó\", \"ÁÑò\", \"ÁÑô\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50420, 768, padding_idx=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancient_chinese_model.resize_token_embeddings(len(ancient_chinese_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NomBertTokenizer\\\\tokenizer_config.json',\n",
       " 'NomBertTokenizer\\\\special_tokens_map.json',\n",
       " 'NomBertTokenizer\\\\vocab.txt',\n",
       " 'NomBertTokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancient_chinese_tokenizer.save_pretrained(\"NomBertTokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ki·ªÉm tra tokenizer m·ªõi c√≥ th·ªÉ encode v√† decode ƒë√∫ng hay kh√¥ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50420, 768, padding_idx=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (BertForMaskedLM, BertTokenizer)\n",
    "# 1. Load the custom tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"NomBertTokenizerv4\")\n",
    "\n",
    "# 2. Load the pre-trained model and resize embeddings\n",
    "model = BertForMaskedLM.from_pretrained(\"Jihuai/bert-ancient-chinese\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust model for the new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong tokenization\n",
      "15 †áÆ Áåâ ËÇù ßëÇ ÂÖ∏ „ô¥ ÂÅú Áúü\n",
      "Wrong decoding. text:  †áÆ Áåâ ËÇù ßëÇ ÂÖ∏ „ô¥ ÂÅú Áúü decoded:  ÊòÜ ‰æØ „ñ´ †ª¥ ¢´ï ®ñ≤ index:  15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"cleaned_data.txt\", \"r\", encoding=\"utf-8\") as f_combined:\n",
    "    clean_data = f_combined.read().splitlines()\n",
    "def preprocess_text(text):\n",
    "    return text.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "for idx,val in enumerate(clean_data[10000:10100]):\n",
    "    #print(\"text: \",val)\n",
    "    try:\n",
    "        encoded = tokenizer.encode(val)\n",
    "    except:\n",
    "        print(\"Wrong tokenization\")\n",
    "        print(idx, val)\n",
    "    decoded = tokenizer.decode(encoded, skip_special_tokens=True)\n",
    "    if preprocess_text(val) != preprocess_text(decoded):\n",
    "        print(\"Wrong decoding. text: \", val, \"decoded: \", decoded,\"index: \", idx)\n",
    "    #print(\"decode: \",val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "\n",
      "Encoded Tokens: ['[CLS]', 'ËÄ®', 'Êñº', 'Âªõ', '‰ºΩ', '\\ue1d5', 'Ë¶∂', 'Ëåπ', '[SEP]']\n",
      "Encoded IDs: [101, 5454, 3176, 22735, 850, 29685, 29680, 5765, 102]\n",
      "\n",
      "Decoded Text: [CLS] ËÄ® Êñº Âªõ ‰ºΩ Óáï Ë¶∂ Ëåπ [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Encode the input text using the tokenizer\n",
    "#test_text = \"†áÆ Áåâ ËÇù ßëÇ ÂÖ∏ „ô¥ ÂÅú Áúü\"\n",
    "test_text = \"ËÄ® Êñº Âªõ ‰ºΩ Óáï Ë¶∂ Ëåπ\"\n",
    "encoding = tokenizer.encode(test_text, add_special_tokens=True)\n",
    "print(type(tokenizer))\n",
    "\n",
    "# Print encoded tokens and IDs\n",
    "print(\"\\nEncoded Tokens:\", tokenizer.convert_ids_to_tokens(encoding))\n",
    "print(\"Encoded IDs:\", encoding)\n",
    "\n",
    "# Step 3: Decode back to text\n",
    "decoded_text = tokenizer.decode(encoding, add_special_tokens=True)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
