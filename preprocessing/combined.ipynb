{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước 1: Tiền xử lý dữ liệu đầu vào"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lý dữ liệu từ Nom_monolingual_corpus_CLC và đưa vào thư mục TokenNom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Đường dẫn đến thư mục chứa các file\n",
    "folder_path = '/path/to/your/folder'  # Thay đổi đường dẫn đến thư mục của bạn\n",
    "\n",
    "# Lấy danh sách tất cả các file trong thư mục\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Tạo đường dẫn đầy đủ của file\n",
    "    old_file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Kiểm tra xem có phải file không (bỏ qua thư mục)\n",
    "    if os.path.isfile(old_file_path):\n",
    "        # Chuyển tên file thành không dấu\n",
    "        new_filename = unidecode(filename)\n",
    "\n",
    "        # Tạo đường dẫn mới cho file\n",
    "        new_file_path = os.path.join(folder_path, new_filename)\n",
    "\n",
    "        # Đổi tên file\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"Đã đổi tên: {filename} -> {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unicode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01municode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unidecode\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Hàm kiểm tra xem một dòng có chứa ký tự không phải Unicode không\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_non_unicode\u001b[39m(line):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unicode'"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import os\n",
    "from unicode import unidecode\n",
    "\n",
    "# Hàm kiểm tra xem một dòng có chứa ký tự không phải Unicode không\n",
    "def contains_non_unicode(line):\n",
    "    try:\n",
    "        # Thử chuẩn hóa dòng, nếu xảy ra lỗi, dòng có ký tự không phải Unicode\n",
    "        unicodedata.normalize(\"NFC\", line)\n",
    "        return False\n",
    "    except UnicodeDecodeError:\n",
    "        return True\n",
    "\n",
    "# Hàm xử lý file\n",
    "def normalize_text(file_path, output_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as infile, \\\n",
    "            open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line_num, line in enumerate(infile, start=1):\n",
    "            # Kiểm tra ký tự không phải Unicode\n",
    "            if contains_non_unicode(line):\n",
    "                print(f\"Line {line_num} in file {file_path} contains non-Unicode characters.\")\n",
    "            \n",
    "            # Chuẩn hóa Unicode và ghi vào file output\n",
    "            normalized_line = unicodedata.normalize(\"NFC\", line.strip())\n",
    "            #print(normalized_line)\n",
    "            outfile.write(normalized_line + \"\\n\")\n",
    "            \n",
    "\n",
    "# Đường dẫn thư mục chứa các file cần xử lý\n",
    "dir_path = \"Data/Nom_monolingual_corpus_CLC/van_van\"\n",
    "dir_path2 = \"Data/Nom_monolingual_corpus_CLC/van_xuoi\"\n",
    "out_dir = \"Data/TokenNom/\"\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        output_path = os.path.join(out_dir, \"normalized_\" + filename)\n",
    "        normalize_text(file_path, output_path)\n",
    "for filename in os.listdir(dir_path2):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(dir_path2, filename)\n",
    "        output_path = os.path.join(out_dir, \"normalized_\" + filename)\n",
    "        normalize_text(file_path, output_path)\n",
    "# Đường dẫn vào dữ liệu trên web chunom.org\n",
    "# https://chunom.org/shelf/corpus/    \n",
    "file_path = \"Data/chunom.org.txt\"\n",
    "\n",
    "output_path = os.path.join(out_dir, \"normalized_chunom_org.txt\")\n",
    "normalize_text(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lý riêng cho data từ web nomfoundation (https://nomfoundation.org/nom-project/history-of-greater-vietnam/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nomfoundation_dir = \"Data/nomfoundation_history_of_greater_vietnam/\"\n",
    "# Hàm xử lý file\n",
    "def normalize_text(file_path, output_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as infile, \\\n",
    "            open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line_num, line in enumerate(infile, start=1):\n",
    "            latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "            # Kiểm tra ký tự không phải Unicode\n",
    "            if contains_non_unicode(line):\n",
    "                print(f\"Line {line_num} in file {file_path} contains non-Unicode characters.\")\n",
    "            if not latin_pattern.search(line) and line.strip():\n",
    "                # Chuẩn hóa Unicode và ghi vào file output\n",
    "                normalized_line = unicodedata.normalize(\"NFC\", line.strip())\n",
    "                #print(normalized_line.replace(\".\",\"\"))\n",
    "                outfile.write(normalized_line.replace(\".\",\"\") + \"\\n\")\n",
    "            \n",
    "for filename in os.listdir(nomfoundation_dir):\n",
    "    filename = filename.split(\"/\")[-1]\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(nomfoundation_dir, filename)\n",
    "        output_path = os.path.join(out_dir, \"normalized_\" + filename)\n",
    "        normalize_text(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lý cho data từ NomNaOCR (https://www.kaggle.com/datasets/quandang/nomnaocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "raw_dir = \"Data/Raw/\"\n",
    "# Hàm xử lý file\n",
    "def normalize_text(file_path, output_path):\n",
    "    data = json.load(open(file_path, 'r', encoding='utf-8'))\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for item in data:\n",
    "            line = item[\"text\"]\n",
    "            # Chuẩn hóa Unicode và ghi vào file output\n",
    "            if contains_non_unicode(line):\n",
    "                print(f\"Line {line} in file {file_path} contains non-Unicode characters.\")\n",
    "            split_line = line.split(\".\")\n",
    "            for sentence in split_line:\n",
    "                latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "                if not latin_pattern.search(sentence) and sentence.strip() and len(sentence.strip()) > 1:\n",
    "                    normalized_line = unicodedata.normalize(\"NFC\", sentence.strip())\n",
    "                    #print(normalized_line.replace(\"\\n\",\"\").strip())\n",
    "                    outfile.write(normalized_line.replace(\".\",\"\") + \"\\n\")\n",
    "        \n",
    "\n",
    "for folder_name in os.listdir(raw_dir):\n",
    "    folder_path = os.path.join(raw_dir, folder_name)\n",
    "    data_path = os.path.join(folder_path, \"automa.json\")\n",
    "    normalize_text(data_path, os.path.join(out_dir, \"normalized_\" + folder_name + \".txt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Làm sạch dữ liệu và xử lý các trường hợp đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Xóa dấu -\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    # Xóa khoảng trắng ở đầu và cuối dòng\n",
    "    text = text.strip()\n",
    "    # Xóa 〇 ở đầu và cuối dòng\n",
    "    if text.endswith(\"〇\"):\n",
    "        text = text[:-1]\n",
    "    if text.startswith(\"〇\"):\n",
    "        text = text[1:]\n",
    "    # Xóa nội dung trong []\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    # Xóa ký tự đặc biệt\n",
    "    text = re.sub(r\"[>!@#\\$%\\^&\\*\\(\\)_\\+\\=\\[\\]\\{\\};:'\\\",<>\\?/\\\\|~`]\", \"\", text)\n",
    "    # Tách từng ký tự không phải khoảng cách và ghép lại với khoảng cách ở giữa\n",
    "    text = \" \".join(char for char in text if not char.isspace())\n",
    "    return text\n",
    "def check_vietnamese(text):\n",
    "    # Tạo pattern để kiểm tra xem một dòng có chứa ít nhất một ký tự tiếng Việt không\n",
    "    vietnamese_pattern = re.compile(r'[a-z0-9A-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễếệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]')\n",
    "    \n",
    "    # Nếu dòng chứa ít nhất một ký tự tiếng Việt thì trả về True (Câu muốn tìm ở đây là Đ a ị)\n",
    "    if vietnamese_pattern.search(text):\n",
    "        return True\n",
    "    return False\n",
    "dir_path = \"Data/TokenNom\"\n",
    "output_file = \"ProcessedData/cleaned_data.txt\"\n",
    "seen_lines = {}  # Từ điển để lưu các dòng đã gặp và tệp gốc của chúng\n",
    "count_same_line = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as output:\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # Làm sạch dòng trước khi kiểm tra\n",
    "                line = clean_text(line)\n",
    "                # Chỉ xử lý các dòng có độ dài > 2 từ\n",
    "                if len(line.split()) > 2 and check_vietnamese(line) == False:\n",
    "                    if line in seen_lines:\n",
    "                        #print(f\"Dòng bị trùng: '{line.strip()}' (đã xuất hiện ở file: {seen_lines[line]})\")\n",
    "                        count_same_line += 1\n",
    "                    else:\n",
    "                        # Ghi dòng mới vào output và thêm vào seen_lines\n",
    "                        output.write(line + '\\n')\n",
    "                        seen_lines[line] = filename  # Ghi nhận file đầu tiên chứa dòng này\n",
    "\n",
    "#print(f\"Tổng số dòng bị trùng: {count_same_line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước 2: Mở rộng tập từng vựng (vocab.txt) của bert ancient chinese với chữ Nôm và tạo tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thu thập bộ từ vựng chữ Nôm từ các dữ liệu hiện có bao gồm QuocNgu_SinoNom_Dic, SinoNom_Similar_Dic, vocabv4.txt và strokes_Han_Nom và bộ dataset đã xây dựng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created successfully: Vocab/vocab_Han_Nom.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Từ vựng từ từ điển Quốc Ngữ - Hán Nôm\n",
    "file1_path = 'Vocab/QuocNgu_SinoNom_Dic.xlsx'\n",
    "file2_path = 'Vocab/SinoNom_Similar_Dic_v2.xlsx'\n",
    "output_file_path = 'Vocab/vocab_Han_Nom.txt'\n",
    "\n",
    "def extract_han_characters(text):\n",
    "    clean_text = text[1:-1]\n",
    "    l = clean_text.split(\",\")\n",
    "    return l\n",
    "\n",
    "file1_data = pd.read_excel(file1_path)\n",
    "sino_nom_chars = file1_data['SinoNom'].dropna().astype(str)\n",
    "cleaned_sino_nom_chars = []\n",
    "for entry in sino_nom_chars:\n",
    "    cleaned_sino_nom_chars.extend(entry)\n",
    "\n",
    "file2_data = pd.read_excel(file2_path)\n",
    "similar_chars_1 = file2_data['Input Character'].dropna().astype(str)\n",
    "cleaned_similar_1 = []\n",
    "for entry in similar_chars_1:\n",
    "    cleaned_similar_1.extend(entry)\n",
    "\n",
    "file2_data = pd.read_excel(file2_path)\n",
    "similar_chars_2 = file2_data['Top 20 Similar Characters'].dropna().astype(str)\n",
    "cleaned_similar_2 = []\n",
    "for entry in similar_chars_2:\n",
    "    l = extract_han_characters(entry)\n",
    "    for char in l:\n",
    "        cleaned_similar_2.extend(char[1:-1])\n",
    "\n",
    "#Từ vựng từ data đã thu thập\n",
    "with open(\"ProcessedData/cleaned_data.txt\", \"r\", encoding=\"utf-8\") as f_combined:\n",
    "    clean_data_chars = list(''.join(f_combined.read().splitlines()))\n",
    "#Từ vựng từ vocab_v4.txt\n",
    "with open(\"Vocab/vocab_v4.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    vocabv4 = list(''.join(lines))\n",
    "# Từ vựng từ strokes Hán Nôm\n",
    "df = pd.read_excel(\"Vocab/strokes_Han_Nom.xlsx\")\n",
    "strokes_chars = list(df.iloc[:,0].dropna().astype(str))\n",
    "\n",
    "#Kết hợp toàn bộ từ vựng để tạo ra tập từ vựng cuối cùng\n",
    "all_cleaned_chars = set(cleaned_sino_nom_chars + cleaned_similar_1 + cleaned_similar_2 + clean_data_chars + vocabv4 + strokes_chars)\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for char in sorted(all_cleaned_chars):\n",
    "        if char != \" \" and char != \"\" and char != \"\\n\" and char not in [\"!@#$%^&*()_+-=[]{};':\\\",.<>/?\\\\|~`\"]:\n",
    "            f.write(f\"{char}\\n\")\n",
    "\n",
    "print(f\"File created successfully: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32354\n",
      "18971\n",
      "7168\n"
     ]
    }
   ],
   "source": [
    "#Kiểm tra kích thước của các tập vocab\n",
    "with open(\"Vocab/vocab_Han_Nom.txt\",'r',encoding=\"utf-8\") as f:\n",
    "    vocab_Nom = f.read()\n",
    "\n",
    "print(len(vocab_Nom.split(\"\\n\")))\n",
    "print(len(vocabv4))\n",
    "print(len(strokes_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước 3: Tạo special list chứa các kí tự không nằm trong vocab ban đầu của bert-ancient-chinese và thêm vào vocab. Cuối cùng lưu lại tokenizer mới"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lưu ý: Cách dưới đây sẽ được thay bằng add_tokens.ipynb bên dưới chỉ là bản thử nghiệm đầu tiên có thể encode và decode được nhưng không thể MASK được các ký tự"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38208\n",
      "32353\n",
      "Nom vocab after filtering Han char:  12216\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import os\n",
    "\n",
    "# Định nghĩa mô hình BERT tiền huấn luyện\n",
    "pretrained_model_name = 'Jihuai/bert-ancient-chinese'\n",
    "\n",
    "# Load mô hình và tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Lấy toàn bộ vocabulary\n",
    "vocab_model = tokenizer.get_vocab().keys()\n",
    "\n",
    "#Hàm xử lý tập vocab_Han_Nom tự tạo\n",
    "def preprocess(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(i.strip().replace('\\n',''))\n",
    "    return l\n",
    "# Load tập vocab_Han_Nom\n",
    "with open('Vocab/vocab_Han_Nom.txt','r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    vocab_Han_Nom = preprocess(data)\n",
    "\n",
    "print(len(vocab_model))\n",
    "print(len(vocab_Han_Nom))\n",
    "#Tạo special tokens\n",
    "special_list = list(set(vocab_Han_Nom) - set(vocab_model))\n",
    "print(\"Nom vocab after filtering Han char: \",len(special_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tokenizer\\\\tokenizer_config.json',\n",
       " 'Tokenizer\\\\special_tokens_map.json',\n",
       " 'Tokenizer\\\\vocab.txt',\n",
       " 'Tokenizer\\\\added_tokens.json',\n",
       " 'Tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Định nghĩa các special tokens\n",
    "\n",
    "special_tokens = {'additional_special_tokens': special_list}\n",
    "\n",
    "# Thêm các special tokens vào tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens_dict=special_tokens)\n",
    "\n",
    "# Thay đổi kích thước của embedding layer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Lưu mô hình và tokenizer\n",
    "tokenizer_save_path = \"Tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kiểm tra encode và decode của tokenizer mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer)\n",
    "import torch\n",
    "from transformers.tokenization_utils import AddedToken\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "sinoNom_tokenizer = AutoTokenizer.from_pretrained('Jihuai/bert-ancient-chinese')\n",
    "#sinoNom_model = BertForMaskedLM.from_pretrained('Jihuai/bert-ancient-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Tokens: ['[CLS]', '𣘃', '豆', '\\U000f1e6b', '挭', '\\U000f1e81', '\\U000f1e81', '[SEP]']\n",
      "Encoded IDs: [101, 47858, 6486, 44258, 35319, 40613, 40613, 102]\n",
      "\n",
      "Decoded Text: [CLS] 𣘃 豆 󱹫 挭 󱺁 󱺁 [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_test = AutoTokenizer.from_pretrained(\"Tokenizer\")\n",
    "\n",
    "test_text = \"𣘃 豆 󱹫 挭 󱺁 󱺁\"\n",
    "\n",
    "# Step 2: Encode the input text using the tokenizer\n",
    "encoding = tokenizer_test.encode(test_text, add_special_tokens=True)  \n",
    "\n",
    "# Print encoded tokens and IDs\n",
    "print(\"\\nEncoded Tokens:\", tokenizer_test.convert_ids_to_tokens(encoding))\n",
    "print(\"Encoded IDs:\", encoding)\n",
    "\n",
    "# Step 3: Decode back to text\n",
    "decoded_text = tokenizer_test.decode(encoding, add_special_tokens=True)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng lại data bằng cách ghép các câu theo max_length với tokenizer mới"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lưu ý: Tokenizer được lấy từ NomBertTokenizer tạo từ file add_tokens.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số đoạn văn: 8036\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load tokenizer và cấu hình\n",
    "sinoNom_tokenizer = BertTokenizer.from_pretrained('NomBertTokenizer')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.strip()\n",
    "    if text.endswith(\"〇\"):\n",
    "        text = text[:-1]\n",
    "    if text.startswith(\"〇\"):\n",
    "        text = text[1:]\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"[>!@#\\\\$%\\^&\\*\\(\\)_\\+\\=\\[\\]\\{\\};:'\\\",<>\\?/\\\\|~`]+\", \"\", text)\n",
    "    text = \" \".join(char for char in text if not char.isspace())\n",
    "    return text\n",
    "\n",
    "def check_vietnamese(text):\n",
    "    # Tạo pattern để kiểm tra xem một dòng có chứa ít nhất một ký tự tiếng Việt không\n",
    "    vietnamese_pattern = re.compile(r'[a-z0-9A-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễếệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]')\n",
    "    \n",
    "    # Nếu dòng chứa ít nhất một ký tự tiếng Việt thì trả về True (Câu muốn tìm ở đây là Đ a ị)\n",
    "    if vietnamese_pattern.search(text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def tokenize_length(text, tokenizer):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "dir_path = \"Data/TokenNom\"\n",
    "output_file = \"ProcessedData/combined_cleaned_datav2.txt\"\n",
    "max_len = 248\n",
    "current_segment = \"\"\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = clean_text(line)\n",
    "            if len(line.split()) <= 2 or check_vietnamese(line):\n",
    "                continue\n",
    "\n",
    "            new_segment = current_segment + (\" [SEP] \" if current_segment else \"\") + line\n",
    "\n",
    "            try:\n",
    "                if tokenize_length(new_segment, sinoNom_tokenizer) > max_len:\n",
    "                    if current_segment:  # Đảm bảo không ghi đoạn rỗng\n",
    "                        data.append(current_segment)\n",
    "                    current_segment = line\n",
    "                else:\n",
    "                    current_segment = new_segment\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý dòng: {line}, {e}\")\n",
    "\n",
    "if current_segment:\n",
    "    data.append(current_segment)\n",
    "\n",
    "# Loại bỏ các đoạn trùng lặp\n",
    "unique_data = list(dict.fromkeys(data))\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as output:\n",
    "    for segment in unique_data:\n",
    "        output.write(segment + '\\n')\n",
    "\n",
    "print(f\"Tổng số đoạn văn: {len(unique_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
