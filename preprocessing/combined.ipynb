{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ Nom_monolingual_corpus_CLC v√† ƒë∆∞a v√†o th∆∞ m·ª•c TokenNom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a c√°c file\n",
    "folder_path = '/path/to/your/folder'  # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c c·ªßa b·∫°n\n",
    "\n",
    "# L·∫•y danh s√°ch t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c\n",
    "for filename in os.listdir(folder_path):\n",
    "    # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß c·ªßa file\n",
    "    old_file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Ki·ªÉm tra xem c√≥ ph·∫£i file kh√¥ng (b·ªè qua th∆∞ m·ª•c)\n",
    "    if os.path.isfile(old_file_path):\n",
    "        # Chuy·ªÉn t√™n file th√†nh kh√¥ng d·∫•u\n",
    "        new_filename = unidecode(filename)\n",
    "\n",
    "        # T·∫°o ƒë∆∞·ªùng d·∫´n m·ªõi cho file\n",
    "        new_file_path = os.path.join(folder_path, new_filename)\n",
    "\n",
    "        # ƒê·ªïi t√™n file\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"ƒê√£ ƒë·ªïi t√™n: {filename} -> {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unicode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01municode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unidecode\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# H√†m ki·ªÉm tra xem m·ªôt d√≤ng c√≥ ch·ª©a k√Ω t·ª± kh√¥ng ph·∫£i Unicode kh√¥ng\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_non_unicode\u001b[39m(line):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unicode'"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import os\n",
    "from unicode import unidecode\n",
    "\n",
    "# H√†m ki·ªÉm tra xem m·ªôt d√≤ng c√≥ ch·ª©a k√Ω t·ª± kh√¥ng ph·∫£i Unicode kh√¥ng\n",
    "def contains_non_unicode(line):\n",
    "    try:\n",
    "        # Th·ª≠ chu·∫©n h√≥a d√≤ng, n·∫øu x·∫£y ra l·ªói, d√≤ng c√≥ k√Ω t·ª± kh√¥ng ph·∫£i Unicode\n",
    "        unicodedata.normalize(\"NFC\", line)\n",
    "        return False\n",
    "    except UnicodeDecodeError:\n",
    "        return True\n",
    "\n",
    "# H√†m x·ª≠ l√Ω file\n",
    "def normalize_text(file_path, output_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as infile, \\\n",
    "            open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line_num, line in enumerate(infile, start=1):\n",
    "            # Ki·ªÉm tra k√Ω t·ª± kh√¥ng ph·∫£i Unicode\n",
    "            if contains_non_unicode(line):\n",
    "                print(f\"Line {line_num} in file {file_path} contains non-Unicode characters.\")\n",
    "            \n",
    "            # Chu·∫©n h√≥a Unicode v√† ghi v√†o file output\n",
    "            normalized_line = unicodedata.normalize(\"NFC\", line.strip())\n",
    "            #print(normalized_line)\n",
    "            outfile.write(normalized_line + \"\\n\")\n",
    "            \n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a c√°c file c·∫ßn x·ª≠ l√Ω\n",
    "dir_path = \"Data/Nom_monolingual_corpus_CLC/van_van\"\n",
    "dir_path2 = \"Data/Nom_monolingual_corpus_CLC/van_xuoi\"\n",
    "out_dir = \"Data/TokenNom/\"\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        output_path = os.path.join(out_dir, \"normalized_\" + filename)\n",
    "        normalize_text(file_path, output_path)\n",
    "for filename in os.listdir(dir_path2):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(dir_path2, filename)\n",
    "        output_path = os.path.join(out_dir, \"normalized_\" + filename)\n",
    "        normalize_text(file_path, output_path)\n",
    "# ƒê∆∞·ªùng d·∫´n v√†o d·ªØ li·ªáu tr√™n web chunom.org\n",
    "# https://chunom.org/shelf/corpus/    \n",
    "file_path = \"Data/chunom.org.txt\"\n",
    "\n",
    "output_path = os.path.join(out_dir, \"normalized_chunom_org.txt\")\n",
    "normalize_text(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X·ª≠ l√Ω ri√™ng cho data t·ª´ web nomfoundation (https://nomfoundation.org/nom-project/history-of-greater-vietnam/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nomfoundation_dir = \"Data/nomfoundation_history_of_greater_vietnam/\"\n",
    "# H√†m x·ª≠ l√Ω file\n",
    "def normalize_text(file_path, output_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as infile, \\\n",
    "            open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line_num, line in enumerate(infile, start=1):\n",
    "            latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "            # Ki·ªÉm tra k√Ω t·ª± kh√¥ng ph·∫£i Unicode\n",
    "            if contains_non_unicode(line):\n",
    "                print(f\"Line {line_num} in file {file_path} contains non-Unicode characters.\")\n",
    "            if not latin_pattern.search(line) and line.strip():\n",
    "                # Chu·∫©n h√≥a Unicode v√† ghi v√†o file output\n",
    "                normalized_line = unicodedata.normalize(\"NFC\", line.strip())\n",
    "                #print(normalized_line.replace(\".\",\"\"))\n",
    "                outfile.write(normalized_line.replace(\".\",\"\") + \"\\n\")\n",
    "            \n",
    "for filename in os.listdir(nomfoundation_dir):\n",
    "    filename = filename.split(\"/\")[-1]\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(nomfoundation_dir, filename)\n",
    "        output_path = os.path.join(out_dir, \"normalized_\" + filename)\n",
    "        normalize_text(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X·ª≠ l√Ω cho data t·ª´ NomNaOCR (https://www.kaggle.com/datasets/quandang/nomnaocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "raw_dir = \"Data/Raw/\"\n",
    "# H√†m x·ª≠ l√Ω file\n",
    "def normalize_text(file_path, output_path):\n",
    "    data = json.load(open(file_path, 'r', encoding='utf-8'))\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for item in data:\n",
    "            line = item[\"text\"]\n",
    "            # Chu·∫©n h√≥a Unicode v√† ghi v√†o file output\n",
    "            if contains_non_unicode(line):\n",
    "                print(f\"Line {line} in file {file_path} contains non-Unicode characters.\")\n",
    "            split_line = line.split(\".\")\n",
    "            for sentence in split_line:\n",
    "                latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "                if not latin_pattern.search(sentence) and sentence.strip() and len(sentence.strip()) > 1:\n",
    "                    normalized_line = unicodedata.normalize(\"NFC\", sentence.strip())\n",
    "                    #print(normalized_line.replace(\"\\n\",\"\").strip())\n",
    "                    outfile.write(normalized_line.replace(\".\",\"\") + \"\\n\")\n",
    "        \n",
    "\n",
    "for folder_name in os.listdir(raw_dir):\n",
    "    folder_path = os.path.join(raw_dir, folder_name)\n",
    "    data_path = os.path.join(folder_path, \"automa.json\")\n",
    "    normalize_text(data_path, os.path.join(out_dir, \"normalized_\" + folder_name + \".txt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L√†m s·∫°ch d·ªØ li·ªáu v√† x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # X√≥a d·∫•u -\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    # X√≥a kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu v√† cu·ªëi d√≤ng\n",
    "    text = text.strip()\n",
    "    # X√≥a „Äá ·ªü ƒë·∫ßu v√† cu·ªëi d√≤ng\n",
    "    if text.endswith(\"„Äá\"):\n",
    "        text = text[:-1]\n",
    "    if text.startswith(\"„Äá\"):\n",
    "        text = text[1:]\n",
    "    # X√≥a n·ªôi dung trong []\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    # X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    text = re.sub(r\"[>!@#\\$%\\^&\\*\\(\\)_\\+\\=\\[\\]\\{\\};:'\\\",<>\\?/\\\\|~`]\", \"\", text)\n",
    "    # T√°ch t·ª´ng k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng c√°ch v√† gh√©p l·∫°i v·ªõi kho·∫£ng c√°ch ·ªü gi·ªØa\n",
    "    text = \" \".join(char for char in text if not char.isspace())\n",
    "    return text\n",
    "def check_vietnamese(text):\n",
    "    # T·∫°o pattern ƒë·ªÉ ki·ªÉm tra xem m·ªôt d√≤ng c√≥ ch·ª©a √≠t nh·∫•t m·ªôt k√Ω t·ª± ti·∫øng Vi·ªát kh√¥ng\n",
    "    vietnamese_pattern = re.compile(r'[a-z0-9A-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·∫ø·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]')\n",
    "    \n",
    "    # N·∫øu d√≤ng ch·ª©a √≠t nh·∫•t m·ªôt k√Ω t·ª± ti·∫øng Vi·ªát th√¨ tr·∫£ v·ªÅ True (C√¢u mu·ªën t√¨m ·ªü ƒë√¢y l√† ƒê a ·ªã)\n",
    "    if vietnamese_pattern.search(text):\n",
    "        return True\n",
    "    return False\n",
    "dir_path = \"Data/TokenNom\"\n",
    "output_file = \"ProcessedData/cleaned_data.txt\"\n",
    "seen_lines = {}  # T·ª´ ƒëi·ªÉn ƒë·ªÉ l∆∞u c√°c d√≤ng ƒë√£ g·∫∑p v√† t·ªáp g·ªëc c·ªßa ch√∫ng\n",
    "count_same_line = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as output:\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # L√†m s·∫°ch d√≤ng tr∆∞·ªõc khi ki·ªÉm tra\n",
    "                line = clean_text(line)\n",
    "                # Ch·ªâ x·ª≠ l√Ω c√°c d√≤ng c√≥ ƒë·ªô d√†i > 2 t·ª´\n",
    "                if len(line.split()) > 2 and check_vietnamese(line) == False:\n",
    "                    if line in seen_lines:\n",
    "                        #print(f\"D√≤ng b·ªã tr√πng: '{line.strip()}' (ƒë√£ xu·∫•t hi·ªán ·ªü file: {seen_lines[line]})\")\n",
    "                        count_same_line += 1\n",
    "                    else:\n",
    "                        # Ghi d√≤ng m·ªõi v√†o output v√† th√™m v√†o seen_lines\n",
    "                        output.write(line + '\\n')\n",
    "                        seen_lines[line] = filename  # Ghi nh·∫≠n file ƒë·∫ßu ti√™n ch·ª©a d√≤ng n√†y\n",
    "\n",
    "#print(f\"T·ªïng s·ªë d√≤ng b·ªã tr√πng: {count_same_line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: M·ªü r·ªông t·∫≠p t·ª´ng v·ª±ng (vocab.txt) c·ªßa bert ancient chinese v·ªõi ch·ªØ N√¥m v√† t·∫°o tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thu th·∫≠p b·ªô t·ª´ v·ª±ng ch·ªØ N√¥m t·ª´ c√°c d·ªØ li·ªáu hi·ªán c√≥ bao g·ªìm QuocNgu_SinoNom_Dic, SinoNom_Similar_Dic, vocabv4.txt v√† strokes_Han_Nom v√† b·ªô dataset ƒë√£ x√¢y d·ª±ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created successfully: Vocab/vocab_Han_Nom.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#T·ª´ v·ª±ng t·ª´ t·ª´ ƒëi·ªÉn Qu·ªëc Ng·ªØ - H√°n N√¥m\n",
    "file1_path = 'Vocab/QuocNgu_SinoNom_Dic.xlsx'\n",
    "file2_path = 'Vocab/SinoNom_Similar_Dic_v2.xlsx'\n",
    "output_file_path = 'Vocab/vocab_Han_Nom.txt'\n",
    "\n",
    "def extract_han_characters(text):\n",
    "    clean_text = text[1:-1]\n",
    "    l = clean_text.split(\",\")\n",
    "    return l\n",
    "\n",
    "file1_data = pd.read_excel(file1_path)\n",
    "sino_nom_chars = file1_data['SinoNom'].dropna().astype(str)\n",
    "cleaned_sino_nom_chars = []\n",
    "for entry in sino_nom_chars:\n",
    "    cleaned_sino_nom_chars.extend(entry)\n",
    "\n",
    "file2_data = pd.read_excel(file2_path)\n",
    "similar_chars_1 = file2_data['Input Character'].dropna().astype(str)\n",
    "cleaned_similar_1 = []\n",
    "for entry in similar_chars_1:\n",
    "    cleaned_similar_1.extend(entry)\n",
    "\n",
    "file2_data = pd.read_excel(file2_path)\n",
    "similar_chars_2 = file2_data['Top 20 Similar Characters'].dropna().astype(str)\n",
    "cleaned_similar_2 = []\n",
    "for entry in similar_chars_2:\n",
    "    l = extract_han_characters(entry)\n",
    "    for char in l:\n",
    "        cleaned_similar_2.extend(char[1:-1])\n",
    "\n",
    "#T·ª´ v·ª±ng t·ª´ data ƒë√£ thu th·∫≠p\n",
    "with open(\"ProcessedData/cleaned_data.txt\", \"r\", encoding=\"utf-8\") as f_combined:\n",
    "    clean_data_chars = list(''.join(f_combined.read().splitlines()))\n",
    "#T·ª´ v·ª±ng t·ª´ vocab_v4.txt\n",
    "with open(\"Vocab/vocab_v4.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    vocabv4 = list(''.join(lines))\n",
    "# T·ª´ v·ª±ng t·ª´ strokes H√°n N√¥m\n",
    "df = pd.read_excel(\"Vocab/strokes_Han_Nom.xlsx\")\n",
    "strokes_chars = list(df.iloc[:,0].dropna().astype(str))\n",
    "\n",
    "#K·∫øt h·ª£p to√†n b·ªô t·ª´ v·ª±ng ƒë·ªÉ t·∫°o ra t·∫≠p t·ª´ v·ª±ng cu·ªëi c√πng\n",
    "all_cleaned_chars = set(cleaned_sino_nom_chars + cleaned_similar_1 + cleaned_similar_2 + clean_data_chars + vocabv4 + strokes_chars)\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for char in sorted(all_cleaned_chars):\n",
    "        if char != \" \" and char != \"\" and char != \"\\n\" and char not in [\"!@#$%^&*()_+-=[]{};':\\\",.<>/?\\\\|~`\"]:\n",
    "            f.write(f\"{char}\\n\")\n",
    "\n",
    "print(f\"File created successfully: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32354\n",
      "18971\n",
      "7168\n"
     ]
    }
   ],
   "source": [
    "#Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa c√°c t·∫≠p vocab\n",
    "with open(\"Vocab/vocab_Han_Nom.txt\",'r',encoding=\"utf-8\") as f:\n",
    "    vocab_Nom = f.read()\n",
    "\n",
    "print(len(vocab_Nom.split(\"\\n\")))\n",
    "print(len(vocabv4))\n",
    "print(len(strokes_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: T·∫°o special list ch·ª©a c√°c k√≠ t·ª± kh√¥ng n·∫±m trong vocab ban ƒë·∫ßu c·ªßa bert-ancient-chinese v√† th√™m v√†o vocab. Cu·ªëi c√πng l∆∞u l·∫°i tokenizer m·ªõi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L∆∞u √Ω: C√°ch d∆∞·ªõi ƒë√¢y s·∫Ω ƒë∆∞·ª£c thay b·∫±ng add_tokens.ipynb b√™n d∆∞·ªõi ch·ªâ l√† b·∫£n th·ª≠ nghi·ªám ƒë·∫ßu ti√™n c√≥ th·ªÉ encode v√† decode ƒë∆∞·ª£c nh∆∞ng kh√¥ng th·ªÉ MASK ƒë∆∞·ª£c c√°c k√Ω t·ª±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38208\n",
      "32353\n",
      "Nom vocab after filtering Han char:  12216\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import os\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a m√¥ h√¨nh BERT ti·ªÅn hu·∫•n luy·ªán\n",
    "pretrained_model_name = 'Jihuai/bert-ancient-chinese'\n",
    "\n",
    "# Load m√¥ h√¨nh v√† tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# L·∫•y to√†n b·ªô vocabulary\n",
    "vocab_model = tokenizer.get_vocab().keys()\n",
    "\n",
    "#H√†m x·ª≠ l√Ω t·∫≠p vocab_Han_Nom t·ª± t·∫°o\n",
    "def preprocess(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(i.strip().replace('\\n',''))\n",
    "    return l\n",
    "# Load t·∫≠p vocab_Han_Nom\n",
    "with open('Vocab/vocab_Han_Nom.txt','r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    vocab_Han_Nom = preprocess(data)\n",
    "\n",
    "print(len(vocab_model))\n",
    "print(len(vocab_Han_Nom))\n",
    "#T·∫°o special tokens\n",
    "special_list = list(set(vocab_Han_Nom) - set(vocab_model))\n",
    "print(\"Nom vocab after filtering Han char: \",len(special_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tokenizer\\\\tokenizer_config.json',\n",
       " 'Tokenizer\\\\special_tokens_map.json',\n",
       " 'Tokenizer\\\\vocab.txt',\n",
       " 'Tokenizer\\\\added_tokens.json',\n",
       " 'Tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a c√°c special tokens\n",
    "\n",
    "special_tokens = {'additional_special_tokens': special_list}\n",
    "\n",
    "# Th√™m c√°c special tokens v√†o tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens_dict=special_tokens)\n",
    "\n",
    "# Thay ƒë·ªïi k√≠ch th∆∞·ªõc c·ªßa embedding layer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh v√† tokenizer\n",
    "tokenizer_save_path = \"Tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ki·ªÉm tra encode v√† decode c·ªßa tokenizer m·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer)\n",
    "import torch\n",
    "from transformers.tokenization_utils import AddedToken\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "sinoNom_tokenizer = AutoTokenizer.from_pretrained('Jihuai/bert-ancient-chinese')\n",
    "#sinoNom_model = BertForMaskedLM.from_pretrained('Jihuai/bert-ancient-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Tokens: ['[CLS]', '£òÉ', 'Ë±Ü', '\\U000f1e6b', 'Êå≠', '\\U000f1e81', '\\U000f1e81', '[SEP]']\n",
      "Encoded IDs: [101, 47858, 6486, 44258, 35319, 40613, 40613, 102]\n",
      "\n",
      "Decoded Text: [CLS] £òÉ Ë±Ü Û±π´ Êå≠ Û±∫Å Û±∫Å [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_test = AutoTokenizer.from_pretrained(\"Tokenizer\")\n",
    "\n",
    "test_text = \"£òÉ Ë±Ü Û±π´ Êå≠ Û±∫Å Û±∫Å\"\n",
    "\n",
    "# Step 2: Encode the input text using the tokenizer\n",
    "encoding = tokenizer_test.encode(test_text, add_special_tokens=True)  \n",
    "\n",
    "# Print encoded tokens and IDs\n",
    "print(\"\\nEncoded Tokens:\", tokenizer_test.convert_ids_to_tokens(encoding))\n",
    "print(\"Encoded IDs:\", encoding)\n",
    "\n",
    "# Step 3: Decode back to text\n",
    "decoded_text = tokenizer_test.decode(encoding, add_special_tokens=True)\n",
    "print(\"\\nDecoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X√¢y d·ª±ng l·∫°i data b·∫±ng c√°ch gh√©p c√°c c√¢u theo max_length v·ªõi tokenizer m·ªõi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L∆∞u √Ω: Tokenizer ƒë∆∞·ª£c l·∫•y t·ª´ NomBertTokenizer t·∫°o t·ª´ file add_tokens.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë ƒëo·∫°n vƒÉn: 8036\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load tokenizer v√† c·∫•u h√¨nh\n",
    "sinoNom_tokenizer = BertTokenizer.from_pretrained('NomBertTokenizer')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.strip()\n",
    "    if text.endswith(\"„Äá\"):\n",
    "        text = text[:-1]\n",
    "    if text.startswith(\"„Äá\"):\n",
    "        text = text[1:]\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"[>!@#\\\\$%\\^&\\*\\(\\)_\\+\\=\\[\\]\\{\\};:'\\\",<>\\?/\\\\|~`]+\", \"\", text)\n",
    "    text = \" \".join(char for char in text if not char.isspace())\n",
    "    return text\n",
    "\n",
    "def check_vietnamese(text):\n",
    "    # T·∫°o pattern ƒë·ªÉ ki·ªÉm tra xem m·ªôt d√≤ng c√≥ ch·ª©a √≠t nh·∫•t m·ªôt k√Ω t·ª± ti·∫øng Vi·ªát kh√¥ng\n",
    "    vietnamese_pattern = re.compile(r'[a-z0-9A-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·∫ø·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]')\n",
    "    \n",
    "    # N·∫øu d√≤ng ch·ª©a √≠t nh·∫•t m·ªôt k√Ω t·ª± ti·∫øng Vi·ªát th√¨ tr·∫£ v·ªÅ True (C√¢u mu·ªën t√¨m ·ªü ƒë√¢y l√† ƒê a ·ªã)\n",
    "    if vietnamese_pattern.search(text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def tokenize_length(text, tokenizer):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "dir_path = \"Data/TokenNom\"\n",
    "output_file = \"ProcessedData/combined_cleaned_datav2.txt\"\n",
    "max_len = 248\n",
    "current_segment = \"\"\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = clean_text(line)\n",
    "            if len(line.split()) <= 2 or check_vietnamese(line):\n",
    "                continue\n",
    "\n",
    "            new_segment = current_segment + (\" [SEP] \" if current_segment else \"\") + line\n",
    "\n",
    "            try:\n",
    "                if tokenize_length(new_segment, sinoNom_tokenizer) > max_len:\n",
    "                    if current_segment:  # ƒê·∫£m b·∫£o kh√¥ng ghi ƒëo·∫°n r·ªóng\n",
    "                        data.append(current_segment)\n",
    "                    current_segment = line\n",
    "                else:\n",
    "                    current_segment = new_segment\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói khi x·ª≠ l√Ω d√≤ng: {line}, {e}\")\n",
    "\n",
    "if current_segment:\n",
    "    data.append(current_segment)\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c ƒëo·∫°n tr√πng l·∫∑p\n",
    "unique_data = list(dict.fromkeys(data))\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as output:\n",
    "    for segment in unique_data:\n",
    "        output.write(segment + '\\n')\n",
    "\n",
    "print(f\"T·ªïng s·ªë ƒëo·∫°n vƒÉn: {len(unique_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
